{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This progream creates concordia index and run information needs queries\n",
    "# NOTE: Simply run the dependencies block and then load stored indeces instead of constructing one (skip code blocks up until the loading section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re,os\n",
    "import nltk\n",
    "import pickle \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "# Set project directory\n",
    "#os.chdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'A3-Q1-2-3-final.ipynb',\n",
       " 'desktop.ini',\n",
       " 'documents-ai',\n",
       " 'documents-ai-2',\n",
       " 'documents-ai-3',\n",
       " 'documents-ai-filtered',\n",
       " 'documents-final',\n",
       " 'Filtering-duplicates.ipynb',\n",
       " 'final_index_concordia',\n",
       " 'Preparing-documents.ipynb',\n",
       " 'Q4-AI-index.ipynb',\n",
       " 'term_freq_concordia',\n",
       " 'urls_run_final.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class custom_scrape():\n",
    "    \n",
    "    docID_link_map = {}\n",
    "    #my_url2 = \"https://www.concordia.ca/research/news.html\"\n",
    "    omit = \"/news/stories\"\n",
    "    doc_counter = 1\n",
    "\n",
    "    def ret_filtered_links(self,url,omit):\n",
    "\n",
    "        link_temp = urlopen(url)\n",
    "        page_html = link_temp.read()\n",
    "        link_temp.close()\n",
    "        page_soup = soup(page_html,\"html.parser\")\n",
    "        research_page = page_soup.findAll('a',href=True)\n",
    "\n",
    "        links = []\n",
    "        filtered_links = []\n",
    "\n",
    "        for i,link in enumerate(research_page):\n",
    "                links.append(link[\"href\"])\n",
    "\n",
    "        for link in links:\n",
    "            if \"/stories\" in link:\n",
    "                if link!=custom_scrape.omit:\n",
    "                    filtered_links.append(link)\n",
    "\n",
    "        for i in range(len(filtered_links)):\n",
    "            filtered_links[i] = \"https://concordia.ca\"+filtered_links[i]\n",
    "            \n",
    "        filtered_links_final = list(set(filtered_links))\n",
    "\n",
    "        return filtered_links_final\n",
    "\n",
    "    def process_filtered_links(self,filtered_links):\n",
    "        for i,link in enumerate(filtered_links):\n",
    "            link_temp = urlopen(link)\n",
    "            page_html = link_temp.read()\n",
    "            link_temp.close()\n",
    "            page_soup = soup(page_html,\"html.parser\")\n",
    "            txt_doc = page_soup.get_text()\n",
    "            os.chdir(\"documents\")\n",
    "            text_file = open(str(custom_scrape.doc_counter), \"w\")\n",
    "            text_file.write(txt_doc)\n",
    "            text_file.close()\n",
    "            os.chdir(\"..\")\n",
    "            custom_scrape.docID_link_map[custom_scrape.doc_counter]=link\n",
    "            custom_scrape.doc_counter+=1\n",
    "            \n",
    "            if custom_scrape.doc_counter%5==0:\n",
    "                print(str(custom_scrape.doc_counter) + \"-doc has been processed!.\")\n",
    "\n",
    "       \n",
    "    \n",
    "    def scrape_docs(self,url,limit = 3):\n",
    "        \n",
    "        for i in range(limit):\n",
    "            if i == 0:\n",
    "                filtered_links = self.ret_filtered_links(url,custom_scrape.omit)\n",
    "                self.process_filtered_links(filtered_links)\n",
    "            else:\n",
    "                new_url = url+\"?page=\"+str(i+1)\n",
    "                filtered_links = self.ret_filtered_links(new_url,custom_scrape.omit)\n",
    "                self.process_filtered_links(filtered_links)\n",
    "                \n",
    "    def scrape_root(self,my_url):\n",
    "        \n",
    "        link = urlopen(my_url)\n",
    "        page_html = link.read()\n",
    "        link.close()\n",
    "        page_soup = soup(page_html,\"html.parser\")\n",
    "        research_page = page_soup.findAll('a',href=True)\n",
    "\n",
    "        soc_media = [\"twitter\",\"linkedin\",\"facebook\",\"instagram\"]\n",
    "        links = []\n",
    "        filtered_links = []\n",
    "        for i,link in enumerate(research_page):\n",
    "                links.append(link[\"href\"])\n",
    "\n",
    "        for link in links:\n",
    "            if \"html\" in link:\n",
    "                filtered_links.append(link)\n",
    "\n",
    "        for media in soc_media:\n",
    "            for link in filtered_links:\n",
    "                if media in link:\n",
    "                    filtered_links.remove(link)\n",
    "\n",
    "        for i in range(len(filtered_links)):\n",
    "            filtered_links[i] = \"https://concordia.ca\"+filtered_links[i]\n",
    "\n",
    "        for i,link in enumerate(filtered_links):\n",
    "\n",
    "            link_temp = urlopen(link)\n",
    "            page_html = link_temp.read()\n",
    "            link_temp.close()\n",
    "            page_soup = soup(page_html,\"html.parser\")\n",
    "            txt_doc = page_soup.get_text()\n",
    "            os.chdir(\"documents\")\n",
    "            text_file = open(str(i+1), \"w\")\n",
    "            text_file.write(txt_doc)\n",
    "            text_file.close()\n",
    "            os.chdir(\"..\")\n",
    "            custom_scrape.docID_link_map[custom_scrape.doc_counter]=link\n",
    "            custom_scrape.doc_counter+=1\n",
    "\n",
    "\"\"\"\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index construction and compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"SPIMI Algorithm class\"\n",
    "\n",
    "class Spimi():\n",
    "    \n",
    "    \"Block counter that will be incremented as the memory gets full\"\n",
    "    b_counter = 1 \n",
    "    \"All the tags to be filtered\"\n",
    "    TAGS = \"<TITLE>,</TITLE>,<TEXT>,</TEXT>,<REUTERS>,</REUTERS>,<DATE>,</DATE>,<TOPICS>,</TOPICS>,<PLACES>,</PLACE>,<PEOPLE>,</PEOPLE>,<COMPANIES>,</COMPANIES>,<UNKNOWN>,</UNKNOWN>,<DATELINE>,</DATELINE>,<BODY>,</BODY>,<AUTHOR>,</AUTHOR>,<ORGS>,</ORGS>,<EXCHANGES>,</EXCHANGES>,<D>,</D>\"\n",
    "    TAGS = TAGS.split(\",\")\n",
    "    Dictionary={}\n",
    "    term_freq = {}\n",
    "    \n",
    "    \n",
    "    \"\"\"Function: Constructor with two main variables, document ID, and \n",
    "       a last_position variable that stores the position of the pointer\n",
    "       in the file being read.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.docID = 1\n",
    "        self.last_pos = 0\n",
    "        \n",
    "    \"Function: sorts a dictionary alphabetically\"\n",
    "    def sort_dict(self,D):\n",
    "        sorted_dict = {}\n",
    "        for key in sorted(D):\n",
    "            sorted_dict[key]=D[key]\n",
    "        return sorted_dict\n",
    "    \n",
    "    \"Writes a block into disk.\"\n",
    "    def write_block(self,outputFile,Dictionary):\n",
    "        with open(outputFile, 'wb') as fp:\n",
    "                        pickle.dump(Dictionary, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    \"Filters the tags in the TAGS variable that show up in a read line from the file.\"            \n",
    "    def filter_tags(self,line):\n",
    "        for tag in Spimi.TAGS:\n",
    "            line=re.sub(tag,\" \", line)\n",
    "        return line\n",
    "    \n",
    "    \"\"\"Function: This is a compression method that helps in removing a word if the number of\n",
    "       identical consecutive characters is greater than two. For example: informmmation\n",
    "    \"\"\"           \n",
    "    def repetitive_check(self,token):\n",
    "        consq_chars = re.findall(r\"((\\w)\\2{2,})\",token)\n",
    "        if len(consq_chars) > 0 :\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    \"Function: This method takes in a token, and updates the dictionary.\"\n",
    "    def updateDict(self,Dictionary,token,stopw_150=False,stem=False):\n",
    "                \n",
    "            token = re.sub(\"[^a-zA-Z]+\", \" \", token)\n",
    "\n",
    "            \n",
    "            \"If a token contains more than one word, it will be omitted. For example: 'america ru' \"\n",
    "            if len(token.split(\" \"))>1: \n",
    "                return\n",
    "            \n",
    "            \"Omits a token that has three consecutive identical characters\"\n",
    "            if self.repetitive_check(token) == True:\n",
    "                return\n",
    "            \n",
    "            \"Checks for stopwords\"           \n",
    "            if stopw_150 == True:\n",
    "                if  token in set(stopwords.words(\"english\")[:150]):\n",
    "                    return \n",
    "                \n",
    "            \"Stems the token\"         \n",
    "            if stem == True:\n",
    "                token = ps.stem(token)\n",
    "            \n",
    "            \"If the token doesn't already exist, add it\"\n",
    "            \"omit tokens that have  length < 3\"\n",
    "            if token not in Dictionary.keys() or token ==\"ai\"and len(token)>2 :          \n",
    "                Dictionary[token] = []\n",
    "                Dictionary[token].append(self.docID)\n",
    "                Spimi.term_freq[token]=1\n",
    "            else:\n",
    "                \"If the token exists, and the corresponding docID isn't already stored, append it to the tokens postings list.\"\n",
    "                if len(token)<3 and token != \"ai\":\n",
    "                    return\n",
    "                #if self.docID not in Dictionary[token]:\n",
    "                Dictionary[token].append(self.docID)\n",
    "                Spimi.term_freq[token]+=1\n",
    "                \n",
    "            l = Dictionary[token]\n",
    "            Dictionary[token] = list(set(l))\n",
    "\n",
    "    \"Function: Spimi algorithm\"\n",
    "    def SPIMI_INVERT(self,txt_file, case_folding=False, no_num=False,stopw_30=False,stopw_150=False,stem=False):\n",
    "        \n",
    "        postings_list = None\n",
    "        output_file = \"block\"+str(self.b_counter) \n",
    "        file = open(txt_file,encoding=\"utf8\",errors=\"ignore\",mode=\"r\")\n",
    "        txt_file=txt_file.split(\".\")[0]\n",
    "        self.docID = int(txt_file)\n",
    " \n",
    "        for line in file:\n",
    "            \n",
    "            \"Filter htmt/xml tags\"    \n",
    "            line = self.filter_tags(line) \n",
    "            \"Filters non-alphanumeric characters\"\n",
    "            line = re.sub(\"\\W+\",\" \", line) \n",
    "            \"Casefold\"\n",
    "            line = line.lower()  \n",
    "            \n",
    "            for word in line.split():\n",
    "\n",
    "                token = word\n",
    "        \n",
    "                \"Update dictionary with each token.\"\n",
    "                self.updateDict(Spimi.Dictionary,token,stopw_150=stopw_150,stem=stem)\n",
    "                \n",
    "\n",
    "        if self.docID%1000==0:\n",
    "\n",
    "            print(\"MEMORY FULL - WRITING TO DISK. - Document:\" + str(self.docID))\n",
    "\n",
    "            Spimi.Dictionary = self.sort_dict(Spimi.Dictionary)\n",
    "            \n",
    "            Spimi.b_counter+=1 \n",
    "\n",
    "            \"Access disk folder\"\n",
    "            os.chdir(\"disk\") \n",
    "            self.write_block(output_file,Spimi.Dictionary)\n",
    "            \"Return to reuters directory\"\n",
    "            os.chdir(\"..\") \n",
    "        elif self.docID == 40207:\n",
    "            \"Access disk folder\"\n",
    "            os.chdir(\"disk\") \n",
    "            self.write_block(output_file,Spimi.Dictionary)\n",
    "            \"Return to reuters directory\"\n",
    "            os.chdir(\"..\") \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing documents to be fed into spimi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'A3-Q1-2-3-final.ipynb',\n",
       " 'A3-Q1-2-3.ipynb',\n",
       " 'desktop.ini',\n",
       " 'documents',\n",
       " 'documents-final',\n",
       " 'documents2',\n",
       " 'documents2-p2',\n",
       " 'Final_Index',\n",
       " 'Preparing-documents.ipynb',\n",
       " 'term_freq',\n",
       " 'Untitled.ipynb',\n",
       " 'Untitled1.ipynb',\n",
       " 'urls_run2.csv',\n",
       " 'urls_run_final.csv']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"documents-final\")\n",
    "documents = os.listdir()\n",
    "documents.remove(\"disk\")\n",
    "#documents.remove(\".DS_Store\")\n",
    "documents = sorted(documents, key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "#documents = sorted(documents)\n",
    "os.chdir(\"..\")\n",
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt',\n",
       " '2.txt',\n",
       " '3.txt',\n",
       " '10.txt',\n",
       " '11.txt',\n",
       " '12.txt',\n",
       " '13.txt',\n",
       " '14.txt',\n",
       " '18.txt',\n",
       " '19.txt',\n",
       " '20.txt',\n",
       " '21.txt',\n",
       " '23.txt',\n",
       " '24.txt',\n",
       " '25.txt',\n",
       " '27.txt',\n",
       " '30.txt',\n",
       " '33.txt',\n",
       " '34.txt',\n",
       " '45.txt',\n",
       " '46.txt',\n",
       " '48.txt',\n",
       " '54.txt',\n",
       " '71.txt',\n",
       " '74.txt',\n",
       " '84.txt',\n",
       " '91.txt',\n",
       " '95.txt',\n",
       " '100.txt',\n",
       " '101.txt',\n",
       " '102.txt',\n",
       " '103.txt',\n",
       " '104.txt',\n",
       " '105.txt',\n",
       " '106.txt',\n",
       " '108.txt',\n",
       " '109.txt',\n",
       " '111.txt',\n",
       " '112.txt',\n",
       " '114.txt',\n",
       " '115.txt',\n",
       " '118.txt',\n",
       " '119.txt',\n",
       " '120.txt',\n",
       " '121.txt',\n",
       " '122.txt',\n",
       " '123.txt',\n",
       " '124.txt',\n",
       " '125.txt',\n",
       " '126.txt',\n",
       " '127.txt',\n",
       " '128.txt',\n",
       " '129.txt',\n",
       " '130.txt',\n",
       " '131.txt',\n",
       " '132.txt',\n",
       " '134.txt',\n",
       " '135.txt',\n",
       " '136.txt',\n",
       " '137.txt',\n",
       " '138.txt',\n",
       " '139.txt',\n",
       " '141.txt',\n",
       " '142.txt',\n",
       " '143.txt',\n",
       " '145.txt',\n",
       " '147.txt',\n",
       " '148.txt',\n",
       " '150.txt',\n",
       " '153.txt',\n",
       " '154.txt',\n",
       " '156.txt',\n",
       " '157.txt',\n",
       " '158.txt',\n",
       " '159.txt',\n",
       " '160.txt',\n",
       " '163.txt',\n",
       " '164.txt',\n",
       " '167.txt',\n",
       " '168.txt',\n",
       " '169.txt',\n",
       " '170.txt',\n",
       " '171.txt',\n",
       " '172.txt',\n",
       " '173.txt',\n",
       " '174.txt',\n",
       " '175.txt',\n",
       " '176.txt',\n",
       " '178.txt',\n",
       " '180.txt',\n",
       " '182.txt',\n",
       " '184.txt',\n",
       " '186.txt',\n",
       " '188.txt',\n",
       " '189.txt',\n",
       " '190.txt',\n",
       " '191.txt',\n",
       " '192.txt',\n",
       " '193.txt',\n",
       " '194.txt',\n",
       " '195.txt',\n",
       " '196.txt',\n",
       " '197.txt',\n",
       " '198.txt',\n",
       " '199.txt',\n",
       " '201.txt',\n",
       " '202.txt',\n",
       " '203.txt',\n",
       " '204.txt',\n",
       " '206.txt',\n",
       " '207.txt',\n",
       " '208.txt',\n",
       " '211.txt',\n",
       " '212.txt',\n",
       " '215.txt',\n",
       " '220.txt',\n",
       " '222.txt',\n",
       " '224.txt',\n",
       " '226.txt',\n",
       " '227.txt',\n",
       " '229.txt',\n",
       " '230.txt',\n",
       " '233.txt',\n",
       " '234.txt',\n",
       " '237.txt',\n",
       " '238.txt',\n",
       " '240.txt',\n",
       " '241.txt',\n",
       " '246.txt',\n",
       " '248.txt',\n",
       " '253.txt',\n",
       " '255.txt',\n",
       " '260.txt',\n",
       " '262.txt',\n",
       " '266.txt',\n",
       " '270.txt',\n",
       " '271.txt',\n",
       " '273.txt',\n",
       " '278.txt',\n",
       " '279.txt',\n",
       " '282.txt',\n",
       " '283.txt',\n",
       " '288.txt',\n",
       " '289.txt',\n",
       " '290.txt',\n",
       " '291.txt',\n",
       " '292.txt',\n",
       " '293.txt',\n",
       " '294.txt',\n",
       " '306.txt',\n",
       " '310.txt',\n",
       " '317.txt',\n",
       " '322.txt',\n",
       " '323.txt',\n",
       " '329.txt',\n",
       " '331.txt',\n",
       " '332.txt',\n",
       " '336.txt',\n",
       " '338.txt',\n",
       " '340.txt',\n",
       " '347.txt',\n",
       " '353.txt',\n",
       " '356.txt',\n",
       " '360.txt',\n",
       " '361.txt',\n",
       " '362.txt',\n",
       " '372.txt',\n",
       " '374.txt',\n",
       " '375.txt',\n",
       " '380.txt',\n",
       " '390.txt',\n",
       " '391.txt',\n",
       " '393.txt',\n",
       " '397.txt',\n",
       " '400.txt',\n",
       " '405.txt',\n",
       " '407.txt',\n",
       " '413.txt',\n",
       " '420.txt',\n",
       " '423.txt',\n",
       " '425.txt',\n",
       " '437.txt',\n",
       " '441.txt',\n",
       " '444.txt',\n",
       " '447.txt',\n",
       " '448.txt',\n",
       " '450.txt',\n",
       " '457.txt',\n",
       " '461.txt',\n",
       " '470.txt',\n",
       " '475.txt',\n",
       " '477.txt',\n",
       " '481.txt',\n",
       " '488.txt',\n",
       " '494.txt',\n",
       " '495.txt',\n",
       " '499.txt',\n",
       " '505.txt',\n",
       " '518.txt',\n",
       " '520.txt',\n",
       " '523.txt',\n",
       " '533.txt',\n",
       " '536.txt',\n",
       " '543.txt',\n",
       " '553.txt',\n",
       " '554.txt',\n",
       " '557.txt',\n",
       " '566.txt',\n",
       " '568.txt',\n",
       " '573.txt',\n",
       " '577.txt',\n",
       " '582.txt',\n",
       " '583.txt',\n",
       " '586.txt',\n",
       " '592.txt',\n",
       " '599.txt',\n",
       " '602.txt',\n",
       " '603.txt',\n",
       " '604.txt',\n",
       " '605.txt',\n",
       " '606.txt',\n",
       " '607.txt',\n",
       " '613.txt',\n",
       " '630.txt',\n",
       " '640.txt',\n",
       " '645.txt',\n",
       " '650.txt',\n",
       " '658.txt',\n",
       " '662.txt',\n",
       " '669.txt',\n",
       " '670.txt',\n",
       " '675.txt',\n",
       " '680.txt',\n",
       " '683.txt',\n",
       " '691.txt',\n",
       " '699.txt',\n",
       " '704.txt',\n",
       " '707.txt',\n",
       " '709.txt',\n",
       " '714.txt',\n",
       " '720.txt',\n",
       " '721.txt',\n",
       " '732.txt',\n",
       " '734.txt',\n",
       " '741.txt',\n",
       " '754.txt',\n",
       " '765.txt',\n",
       " '766.txt',\n",
       " '769.txt',\n",
       " '778.txt',\n",
       " '783.txt',\n",
       " '786.txt',\n",
       " '790.txt',\n",
       " '793.txt',\n",
       " '794.txt',\n",
       " '816.txt',\n",
       " '817.txt',\n",
       " '818.txt',\n",
       " '819.txt',\n",
       " '824.txt',\n",
       " '829.txt',\n",
       " '832.txt',\n",
       " '845.txt',\n",
       " '852.txt',\n",
       " '855.txt',\n",
       " '858.txt',\n",
       " '862.txt',\n",
       " '873.txt',\n",
       " '877.txt',\n",
       " '878.txt',\n",
       " '882.txt',\n",
       " '887.txt',\n",
       " '899.txt',\n",
       " '903.txt',\n",
       " '905.txt',\n",
       " '915.txt',\n",
       " '920.txt',\n",
       " '923.txt',\n",
       " '929.txt',\n",
       " '930.txt',\n",
       " '938.txt',\n",
       " '941.txt',\n",
       " '949.txt',\n",
       " '952.txt',\n",
       " '954.txt',\n",
       " '961.txt',\n",
       " '966.txt',\n",
       " '976.txt',\n",
       " '980.txt',\n",
       " '989.txt',\n",
       " '996.txt',\n",
       " '999.txt',\n",
       " '1000.txt',\n",
       " '1001.txt',\n",
       " '1002.txt',\n",
       " '1003.txt',\n",
       " '1004.txt',\n",
       " '1005.txt',\n",
       " '1006.txt',\n",
       " '1007.txt',\n",
       " '1008.txt',\n",
       " '1009.txt',\n",
       " '1010.txt',\n",
       " '1011.txt',\n",
       " '1012.txt',\n",
       " '1013.txt',\n",
       " '1014.txt',\n",
       " '1015.txt',\n",
       " '1016.txt',\n",
       " '1017.txt',\n",
       " '1018.txt',\n",
       " '1019.txt',\n",
       " '1020.txt',\n",
       " '1021.txt',\n",
       " '1022.txt',\n",
       " '1023.txt',\n",
       " '1024.txt',\n",
       " '1025.txt',\n",
       " '1027.txt',\n",
       " '1028.txt',\n",
       " '1029.txt',\n",
       " '1030.txt',\n",
       " '1031.txt',\n",
       " '1032.txt',\n",
       " '1033.txt',\n",
       " '1034.txt',\n",
       " '1035.txt',\n",
       " '1036.txt',\n",
       " '1037.txt',\n",
       " '1038.txt',\n",
       " '1039.txt',\n",
       " '1040.txt',\n",
       " '1041.txt',\n",
       " '1042.txt',\n",
       " '1043.txt',\n",
       " '1044.txt',\n",
       " '1045.txt',\n",
       " '1046.txt',\n",
       " '1047.txt',\n",
       " '1048.txt',\n",
       " '1049.txt',\n",
       " '1050.txt',\n",
       " '1051.txt',\n",
       " '1052.txt',\n",
       " '1053.txt',\n",
       " '1054.txt',\n",
       " '1056.txt',\n",
       " '1057.txt',\n",
       " '1058.txt',\n",
       " '1059.txt',\n",
       " '1060.txt',\n",
       " '1061.txt',\n",
       " '1062.txt',\n",
       " '1063.txt',\n",
       " '1064.txt',\n",
       " '1065.txt',\n",
       " '1066.txt',\n",
       " '1067.txt',\n",
       " '1069.txt',\n",
       " '1070.txt',\n",
       " '1071.txt',\n",
       " '1072.txt',\n",
       " '1073.txt',\n",
       " '1074.txt',\n",
       " '1075.txt',\n",
       " '1076.txt',\n",
       " '1077.txt',\n",
       " '1078.txt',\n",
       " '1079.txt',\n",
       " '1080.txt',\n",
       " '1081.txt',\n",
       " '1082.txt',\n",
       " '1083.txt',\n",
       " '1084.txt',\n",
       " '1085.txt',\n",
       " '1086.txt',\n",
       " '1087.txt',\n",
       " '1088.txt',\n",
       " '1089.txt',\n",
       " '1090.txt',\n",
       " '1091.txt',\n",
       " '1092.txt',\n",
       " '1093.txt',\n",
       " '1094.txt',\n",
       " '1095.txt',\n",
       " '1096.txt',\n",
       " '1097.txt',\n",
       " '1098.txt',\n",
       " '1099.txt',\n",
       " '1100.txt',\n",
       " '1101.txt',\n",
       " '1103.txt',\n",
       " '1104.txt',\n",
       " '1105.txt',\n",
       " '1106.txt',\n",
       " '1107.txt',\n",
       " '1108.txt',\n",
       " '1109.txt',\n",
       " '1112.txt',\n",
       " '1114.txt',\n",
       " '1116.txt',\n",
       " '1118.txt',\n",
       " '1120.txt',\n",
       " '1121.txt',\n",
       " '1122.txt',\n",
       " '1123.txt',\n",
       " '1124.txt',\n",
       " '1125.txt',\n",
       " '1126.txt',\n",
       " '1127.txt',\n",
       " '1128.txt',\n",
       " '1129.txt',\n",
       " '1130.txt',\n",
       " '1131.txt',\n",
       " '1132.txt',\n",
       " '1133.txt',\n",
       " '1134.txt',\n",
       " '1135.txt',\n",
       " '1136.txt',\n",
       " '1137.txt',\n",
       " '1138.txt',\n",
       " '1139.txt',\n",
       " '1140.txt',\n",
       " '1141.txt',\n",
       " '1142.txt',\n",
       " '1143.txt',\n",
       " '1144.txt',\n",
       " '1145.txt',\n",
       " '1146.txt',\n",
       " '1147.txt',\n",
       " '1149.txt',\n",
       " '1150.txt',\n",
       " '1151.txt',\n",
       " '1152.txt',\n",
       " '1153.txt',\n",
       " '1154.txt',\n",
       " '1155.txt',\n",
       " '1156.txt',\n",
       " '1158.txt',\n",
       " '1159.txt',\n",
       " '1160.txt',\n",
       " '1161.txt',\n",
       " '1162.txt',\n",
       " '1163.txt',\n",
       " '1164.txt',\n",
       " '1165.txt',\n",
       " '1166.txt',\n",
       " '1167.txt',\n",
       " '1168.txt',\n",
       " '1169.txt',\n",
       " '1170.txt',\n",
       " '1171.txt',\n",
       " '1173.txt',\n",
       " '1174.txt',\n",
       " '1175.txt',\n",
       " '1176.txt',\n",
       " '1177.txt',\n",
       " '1179.txt',\n",
       " '1180.txt',\n",
       " '1181.txt',\n",
       " '1182.txt',\n",
       " '1183.txt',\n",
       " '1184.txt',\n",
       " '1185.txt',\n",
       " '1186.txt',\n",
       " '1187.txt',\n",
       " '1188.txt',\n",
       " '1189.txt',\n",
       " '1190.txt',\n",
       " '1191.txt',\n",
       " '1192.txt',\n",
       " '1193.txt',\n",
       " '1194.txt',\n",
       " '1195.txt',\n",
       " '1196.txt',\n",
       " '1197.txt',\n",
       " '1198.txt',\n",
       " '1199.txt',\n",
       " '1200.txt',\n",
       " '1201.txt',\n",
       " '1202.txt',\n",
       " '1203.txt',\n",
       " '1205.txt',\n",
       " '1208.txt',\n",
       " '1209.txt',\n",
       " '1210.txt',\n",
       " '1211.txt',\n",
       " '1212.txt',\n",
       " '1213.txt',\n",
       " '1215.txt',\n",
       " '1216.txt',\n",
       " '1218.txt',\n",
       " '1220.txt',\n",
       " '1221.txt',\n",
       " '1222.txt',\n",
       " '1224.txt',\n",
       " '1225.txt',\n",
       " '1227.txt',\n",
       " '1228.txt',\n",
       " '1230.txt',\n",
       " '1231.txt',\n",
       " '1232.txt',\n",
       " '1233.txt',\n",
       " '1234.txt',\n",
       " '1236.txt',\n",
       " '1237.txt',\n",
       " '1239.txt',\n",
       " '1240.txt',\n",
       " '1241.txt',\n",
       " '1242.txt',\n",
       " '1243.txt',\n",
       " '1245.txt',\n",
       " '1246.txt',\n",
       " '1247.txt',\n",
       " '1248.txt',\n",
       " '1249.txt',\n",
       " '1250.txt',\n",
       " '1251.txt',\n",
       " '1253.txt',\n",
       " '1254.txt',\n",
       " '1255.txt',\n",
       " '1257.txt',\n",
       " '1258.txt',\n",
       " '1260.txt',\n",
       " '1262.txt',\n",
       " '1263.txt',\n",
       " '1264.txt',\n",
       " '1266.txt',\n",
       " '1269.txt',\n",
       " '1270.txt',\n",
       " '1271.txt',\n",
       " '1273.txt',\n",
       " '1274.txt',\n",
       " '1275.txt',\n",
       " '1276.txt',\n",
       " '1277.txt',\n",
       " '1278.txt',\n",
       " '1279.txt',\n",
       " '1280.txt',\n",
       " '1281.txt',\n",
       " '1282.txt',\n",
       " '1283.txt',\n",
       " '1284.txt',\n",
       " '1285.txt',\n",
       " '1287.txt',\n",
       " '1288.txt',\n",
       " '1289.txt',\n",
       " '1291.txt',\n",
       " '1293.txt',\n",
       " '1294.txt',\n",
       " '1295.txt',\n",
       " '1296.txt',\n",
       " '1297.txt',\n",
       " '1300.txt',\n",
       " '1301.txt',\n",
       " '1302.txt',\n",
       " '1304.txt',\n",
       " '1307.txt',\n",
       " '1308.txt',\n",
       " '1309.txt',\n",
       " '1310.txt',\n",
       " '1311.txt',\n",
       " '1312.txt',\n",
       " '1313.txt',\n",
       " '1314.txt',\n",
       " '1315.txt',\n",
       " '1317.txt',\n",
       " '1318.txt',\n",
       " '1322.txt',\n",
       " '1324.txt',\n",
       " '1325.txt',\n",
       " '1327.txt',\n",
       " '1330.txt',\n",
       " '1334.txt',\n",
       " '1336.txt',\n",
       " '1338.txt',\n",
       " '1339.txt',\n",
       " '1341.txt',\n",
       " '1345.txt',\n",
       " '1346.txt',\n",
       " '1347.txt',\n",
       " '1349.txt',\n",
       " '1350.txt',\n",
       " '1351.txt',\n",
       " '1352.txt',\n",
       " '1353.txt',\n",
       " '1354.txt',\n",
       " '1355.txt',\n",
       " '1358.txt',\n",
       " '1359.txt',\n",
       " '1360.txt',\n",
       " '1361.txt',\n",
       " '1362.txt',\n",
       " '1363.txt',\n",
       " '1364.txt',\n",
       " '1365.txt',\n",
       " '1366.txt',\n",
       " '1367.txt',\n",
       " '1368.txt',\n",
       " '1369.txt',\n",
       " '1372.txt',\n",
       " '1374.txt',\n",
       " '1375.txt',\n",
       " '1376.txt',\n",
       " '1377.txt',\n",
       " '1378.txt',\n",
       " '1380.txt',\n",
       " '1381.txt',\n",
       " '1382.txt',\n",
       " '1383.txt',\n",
       " '1384.txt',\n",
       " '1385.txt',\n",
       " '1387.txt',\n",
       " '1389.txt',\n",
       " '1390.txt',\n",
       " '1391.txt',\n",
       " '1393.txt',\n",
       " '1394.txt',\n",
       " '1395.txt',\n",
       " '1396.txt',\n",
       " '1397.txt',\n",
       " '1398.txt',\n",
       " '1401.txt',\n",
       " '1402.txt',\n",
       " '1403.txt',\n",
       " '1404.txt',\n",
       " '1405.txt',\n",
       " '1406.txt',\n",
       " '1408.txt',\n",
       " '1409.txt',\n",
       " '1410.txt',\n",
       " '1412.txt',\n",
       " '1413.txt',\n",
       " '1415.txt',\n",
       " '1417.txt',\n",
       " '1418.txt',\n",
       " '1419.txt',\n",
       " '1420.txt',\n",
       " '1423.txt',\n",
       " '1425.txt',\n",
       " '1427.txt',\n",
       " '1428.txt',\n",
       " '1429.txt',\n",
       " '1430.txt',\n",
       " '1433.txt',\n",
       " '1434.txt',\n",
       " '1435.txt',\n",
       " '1439.txt',\n",
       " '1441.txt',\n",
       " '1442.txt',\n",
       " '1443.txt',\n",
       " '1445.txt',\n",
       " '1446.txt',\n",
       " '1447.txt',\n",
       " '1448.txt',\n",
       " '1449.txt',\n",
       " '1450.txt',\n",
       " '1452.txt',\n",
       " '1453.txt',\n",
       " '1456.txt',\n",
       " '1457.txt',\n",
       " '1460.txt',\n",
       " '1461.txt',\n",
       " '1462.txt',\n",
       " '1464.txt',\n",
       " '1465.txt',\n",
       " '1466.txt',\n",
       " '1467.txt',\n",
       " '1469.txt',\n",
       " '1471.txt',\n",
       " '1474.txt',\n",
       " '1475.txt',\n",
       " '1476.txt',\n",
       " '1477.txt',\n",
       " '1478.txt',\n",
       " '1480.txt',\n",
       " '1481.txt',\n",
       " '1482.txt',\n",
       " '1483.txt',\n",
       " '1484.txt',\n",
       " '1485.txt',\n",
       " '1486.txt',\n",
       " '1487.txt',\n",
       " '1488.txt',\n",
       " '1489.txt',\n",
       " '1492.txt',\n",
       " '1493.txt',\n",
       " '1494.txt',\n",
       " '1495.txt',\n",
       " '1496.txt',\n",
       " '1499.txt',\n",
       " '1500.txt',\n",
       " '1502.txt',\n",
       " '1503.txt',\n",
       " '1506.txt',\n",
       " '1507.txt',\n",
       " '1509.txt',\n",
       " '1511.txt',\n",
       " '1512.txt',\n",
       " '1513.txt',\n",
       " '1514.txt',\n",
       " '1516.txt',\n",
       " '1517.txt',\n",
       " '1518.txt',\n",
       " '1519.txt',\n",
       " '1521.txt',\n",
       " '1524.txt',\n",
       " '1525.txt',\n",
       " '1527.txt',\n",
       " '1528.txt',\n",
       " '1529.txt',\n",
       " '1530.txt',\n",
       " '1531.txt',\n",
       " '1533.txt',\n",
       " '1534.txt',\n",
       " '1535.txt',\n",
       " '1536.txt',\n",
       " '1537.txt',\n",
       " '1538.txt',\n",
       " '1539.txt',\n",
       " '1540.txt',\n",
       " '1541.txt',\n",
       " '1542.txt',\n",
       " '1543.txt',\n",
       " '1545.txt',\n",
       " '1546.txt',\n",
       " '1549.txt',\n",
       " '1550.txt',\n",
       " '1551.txt',\n",
       " '1552.txt',\n",
       " '1553.txt',\n",
       " '1556.txt',\n",
       " '1557.txt',\n",
       " '1558.txt',\n",
       " '1559.txt',\n",
       " '1561.txt',\n",
       " '1562.txt',\n",
       " '1564.txt',\n",
       " '1565.txt',\n",
       " '1568.txt',\n",
       " '1570.txt',\n",
       " '1571.txt',\n",
       " '1574.txt',\n",
       " '1576.txt',\n",
       " '1580.txt',\n",
       " '1582.txt',\n",
       " '1583.txt',\n",
       " '1584.txt',\n",
       " '1585.txt',\n",
       " '1587.txt',\n",
       " '1588.txt',\n",
       " '1589.txt',\n",
       " '1590.txt',\n",
       " '1592.txt',\n",
       " '1594.txt',\n",
       " '1595.txt',\n",
       " '1596.txt',\n",
       " '1597.txt',\n",
       " '1598.txt',\n",
       " '1599.txt',\n",
       " '1600.txt',\n",
       " '1601.txt',\n",
       " '1602.txt',\n",
       " '1605.txt',\n",
       " '1606.txt',\n",
       " '1607.txt',\n",
       " '1608.txt',\n",
       " '1609.txt',\n",
       " '1610.txt',\n",
       " '1611.txt',\n",
       " '1612.txt',\n",
       " '1614.txt',\n",
       " '1615.txt',\n",
       " '1617.txt',\n",
       " '1619.txt',\n",
       " '1622.txt',\n",
       " '1624.txt',\n",
       " '1625.txt',\n",
       " '1626.txt',\n",
       " '1629.txt',\n",
       " '1631.txt',\n",
       " '1632.txt',\n",
       " '1633.txt',\n",
       " '1635.txt',\n",
       " '1636.txt',\n",
       " '1638.txt',\n",
       " '1640.txt',\n",
       " '1641.txt',\n",
       " '1643.txt',\n",
       " '1645.txt',\n",
       " '1646.txt',\n",
       " '1647.txt',\n",
       " '1649.txt',\n",
       " '1650.txt',\n",
       " '1652.txt',\n",
       " '1653.txt',\n",
       " '1655.txt',\n",
       " '1659.txt',\n",
       " '1663.txt',\n",
       " '1665.txt',\n",
       " '1666.txt',\n",
       " '1668.txt',\n",
       " '1669.txt',\n",
       " '1671.txt',\n",
       " '1674.txt',\n",
       " '1675.txt',\n",
       " '1676.txt',\n",
       " '1679.txt',\n",
       " '1681.txt',\n",
       " '1682.txt',\n",
       " '1683.txt',\n",
       " '1684.txt',\n",
       " '1685.txt',\n",
       " '1687.txt',\n",
       " '1688.txt',\n",
       " '1689.txt',\n",
       " '1690.txt',\n",
       " '1691.txt',\n",
       " '1692.txt',\n",
       " '1695.txt',\n",
       " '1696.txt',\n",
       " '1697.txt',\n",
       " '1698.txt',\n",
       " '1699.txt',\n",
       " '1700.txt',\n",
       " '1702.txt',\n",
       " '1703.txt',\n",
       " '1704.txt',\n",
       " '1706.txt',\n",
       " '1708.txt',\n",
       " '1710.txt',\n",
       " '1712.txt',\n",
       " '1713.txt',\n",
       " '1714.txt',\n",
       " '1715.txt',\n",
       " '1716.txt',\n",
       " '1717.txt',\n",
       " '1718.txt',\n",
       " '1719.txt',\n",
       " '1722.txt',\n",
       " '1724.txt',\n",
       " '1725.txt',\n",
       " '1726.txt',\n",
       " '1727.txt',\n",
       " '1728.txt',\n",
       " '1729.txt',\n",
       " '1730.txt',\n",
       " '1731.txt',\n",
       " '1733.txt',\n",
       " '1735.txt',\n",
       " '1736.txt',\n",
       " '1737.txt',\n",
       " '1738.txt',\n",
       " '1740.txt',\n",
       " '1741.txt',\n",
       " '1742.txt',\n",
       " '1743.txt',\n",
       " '1747.txt',\n",
       " '1748.txt',\n",
       " '1750.txt',\n",
       " '1751.txt',\n",
       " '1752.txt',\n",
       " '1754.txt',\n",
       " '1756.txt',\n",
       " '1757.txt',\n",
       " '1758.txt',\n",
       " '1760.txt',\n",
       " '1761.txt',\n",
       " '1763.txt',\n",
       " '1764.txt',\n",
       " '1765.txt',\n",
       " '1767.txt',\n",
       " '1770.txt',\n",
       " '1771.txt',\n",
       " '1773.txt',\n",
       " '1774.txt',\n",
       " '1775.txt',\n",
       " '1776.txt',\n",
       " '1777.txt',\n",
       " '1782.txt',\n",
       " '1786.txt',\n",
       " '1787.txt',\n",
       " '1793.txt',\n",
       " '1794.txt',\n",
       " '1796.txt',\n",
       " '1798.txt',\n",
       " '1799.txt',\n",
       " '1801.txt',\n",
       " '1802.txt',\n",
       " '1803.txt',\n",
       " '1804.txt',\n",
       " '1805.txt',\n",
       " '1806.txt',\n",
       " '1807.txt',\n",
       " '1808.txt',\n",
       " '1810.txt',\n",
       " '1811.txt',\n",
       " '1812.txt',\n",
       " '1813.txt',\n",
       " '1819.txt',\n",
       " '1820.txt',\n",
       " '1821.txt',\n",
       " '1822.txt',\n",
       " '1825.txt',\n",
       " '1826.txt',\n",
       " '1828.txt',\n",
       " '1830.txt',\n",
       " '1831.txt',\n",
       " '1834.txt',\n",
       " '1836.txt',\n",
       " '1837.txt',\n",
       " '1838.txt',\n",
       " '1842.txt',\n",
       " '1843.txt',\n",
       " '1844.txt',\n",
       " '1845.txt',\n",
       " '1847.txt',\n",
       " '1851.txt',\n",
       " '1852.txt',\n",
       " '1855.txt',\n",
       " '1856.txt',\n",
       " '1857.txt',\n",
       " '1858.txt',\n",
       " '1859.txt',\n",
       " '1860.txt',\n",
       " '1862.txt',\n",
       " '1863.txt',\n",
       " '1866.txt',\n",
       " '1868.txt',\n",
       " '1869.txt',\n",
       " '1871.txt',\n",
       " '1872.txt',\n",
       " '1873.txt',\n",
       " '1874.txt',\n",
       " '1875.txt',\n",
       " '1876.txt',\n",
       " '1877.txt',\n",
       " '1878.txt',\n",
       " '1879.txt',\n",
       " '1880.txt',\n",
       " '1881.txt',\n",
       " '1882.txt',\n",
       " '1884.txt',\n",
       " '1885.txt',\n",
       " '1887.txt',\n",
       " '1888.txt',\n",
       " '1892.txt',\n",
       " '1894.txt',\n",
       " '1895.txt',\n",
       " '1897.txt',\n",
       " '1898.txt',\n",
       " '1899.txt',\n",
       " '1900.txt',\n",
       " '1901.txt',\n",
       " '1903.txt',\n",
       " '1904.txt',\n",
       " '1905.txt',\n",
       " '1906.txt',\n",
       " '1912.txt',\n",
       " '1913.txt',\n",
       " '1917.txt',\n",
       " '1919.txt',\n",
       " '1920.txt',\n",
       " '1921.txt',\n",
       " '1922.txt',\n",
       " '1926.txt',\n",
       " '1929.txt',\n",
       " '1930.txt',\n",
       " '1931.txt',\n",
       " '1933.txt',\n",
       " '1936.txt',\n",
       " '1937.txt',\n",
       " '1938.txt',\n",
       " '1940.txt',\n",
       " '1942.txt',\n",
       " '1943.txt',\n",
       " '1944.txt',\n",
       " '1947.txt',\n",
       " '1948.txt',\n",
       " '1949.txt',\n",
       " '1952.txt',\n",
       " '1955.txt',\n",
       " '1958.txt',\n",
       " '1959.txt',\n",
       " '1960.txt',\n",
       " '1961.txt',\n",
       " '1962.txt',\n",
       " '1965.txt',\n",
       " '1966.txt',\n",
       " '1968.txt',\n",
       " '1969.txt',\n",
       " '1970.txt',\n",
       " '1972.txt',\n",
       " '1973.txt',\n",
       " '1974.txt',\n",
       " '1975.txt',\n",
       " '1976.txt',\n",
       " '1977.txt',\n",
       " '1978.txt',\n",
       " '1979.txt',\n",
       " ...]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY FULL - WRITING TO DISK. - Document:1000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:3000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:5000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:10000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:11000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:12000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:13000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:14000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:15000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:16000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:17000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:20000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:24000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:25000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:27000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:32000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:35000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:36000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:37000\n",
      "MEMORY FULL - WRITING TO DISK. - Document:40000\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"documents-final\")\n",
    "spimi_obj = Spimi()\n",
    "for txt_file in documents:\n",
    "     spimi_obj.SPIMI_INVERT(txt_file,stem=False,stopw_150=False)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['block1', 'block2', 'block3', 'block4', 'block5', 'block6', 'block7', 'block8', 'block9', 'block10', 'block11', 'block12', 'block13', 'block14', 'block15', 'block16', 'block17', 'block18', 'block19', 'block20'])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Access disk folder.\"\n",
    "os.chdir(\"disk\") \n",
    "Blocks={}\n",
    "\"Store all blocks in a dictionary called Blocks.\"\n",
    "for i in range(len(os.listdir())):\n",
    "    with open(\"block\"+str(i+1), 'rb') as fp:\n",
    "        block = pickle.load(fp)\n",
    "        Blocks[\"block\"+str(i+1)] = block\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "Blocks.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Function: returns a list of the names of all the blocks that a term exists in.\"\n",
    "def term_block_mapping(term, Blocks):\n",
    "    term_block_mapping = []\n",
    "    for block in Blocks:\n",
    "        if term in Blocks[block].keys():\n",
    "            term_block_mapping.append(block)\n",
    "            \n",
    "    return term_block_mapping\n",
    "\n",
    "\n",
    "Merged_blocks = {}\n",
    "for block in Blocks.keys():\n",
    "    Merged_blocks.update(Blocks[block])\n",
    "print(\"num of terms: \" + str(len(Merged_blocks)))\n",
    "all_terms = list(Merged_blocks.keys())\n",
    "\n",
    "\n",
    "Merged_block = {}\n",
    "Final_index = {}\n",
    "MB_count=1\n",
    "\"\"\"\n",
    "For every term (in sorted list of terms), return all its postings list,\n",
    "merge them, and then store a mapping of that term and its merged postings into a temporary\n",
    "dictionary untill 25000 terms are reached. If 25000 terms are reached, store that temporary\n",
    "dictionary into a final index dictionary, reset the temporary dictionary variable, and repeat\n",
    "the process.\n",
    "\"\"\"   \n",
    "for term in sorted(all_terms):\n",
    "        \n",
    "    term_in_blocks = term_block_mapping(term,Blocks)\n",
    "\n",
    "    Merged_block[term] = []\n",
    "\n",
    "    for block in term_in_blocks:\n",
    "        Merged_block[term].extend(Blocks[block][term])\n",
    "    \n",
    "    l = Merged_block[term]\n",
    "    Merged_block[term] = list(set(l))\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save term freq and final index\n",
    "#s = Spimi()\n",
    "#s.write_block(\"term_freq_concordia\",Spimi.term_freq)\n",
    "#s.write_block(\"final_index_concordia\",Merged_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Final index and term freq instead of obtaining them from running the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run previous code. Just run this block to load Final index in (takes time to compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged_block = open(\"Final_index\")\n",
    "Merged_block = pickle.load(open(\"final_index_concordia\",mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term frequency\n",
    "term_freq = pickle.load(open(\"term_freq_concordia\",mode=\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Another approach to get length of a document.. Counts the number of times a token occured in some document(its length).\"\n",
    "def get_doc_length(doc_id):\n",
    "    \n",
    "    os.chdir(\"documents-final\")\n",
    "    f = open(str(doc_id)+\".txt\",\"r\",encoding=\"utf=8\")\n",
    "    text= f.read()\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "    return len(text.split(\" \"))\n",
    "\n",
    "\"Calculates the average length of all documents using the get_doc_length method.\"\n",
    "def get_collection_length():\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    os.chdir(\"documents-final\")\n",
    "    docs = os.listdir()\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "    for doc in docs:\n",
    "        if doc.split(\".\")[0].isdigit():\n",
    "            total+=get_doc_length(doc)\n",
    "    \n",
    "    \n",
    "        \n",
    "    return total/40211\n",
    "        \n",
    "\n",
    "\"BM25 Ranking formula implementation\"\n",
    "def rank_doc(doc_id,term):\n",
    "        \n",
    "    N = 17393 # Total documents number\n",
    "    k1 = 0.25 # tuning parameter\n",
    "    b = 0.75 # tuning parameter\n",
    "    postings_list = []\n",
    "    \n",
    "    avg_doc_length = 200.04749944045162\n",
    "\n",
    "\n",
    "    doc_length = get_doc_length(doc_id)\n",
    "        \n",
    "    \n",
    "    \"Find term frequency\"\n",
    "    \n",
    "    term_frequency = term_freq[term]\n",
    "    \n",
    "    \"Find document frequency\"\n",
    "    document_frequency = len(Merged_block[term])\n",
    "    \n",
    "    \"Formula steps:\"\n",
    "    \n",
    "    idf = np.log(N/document_frequency)\n",
    "    numerator = (k1+1)*term_frequency\n",
    "    denomenator = k1*((1-b)+ (b*(doc_length/avg_doc_length))) + term_frequency\n",
    "    \n",
    "    ratio = numerator/denomenator\n",
    "    \n",
    "    weight = idf * ratio\n",
    "    \n",
    "    return weight\n",
    "\n",
    "def rank_doc_tfidf(doc_id,term):\n",
    "    \n",
    "    N = 17393 # Total documents number\n",
    "    postings_list = []\n",
    "    \n",
    "    avg_doc_length = 200.04749944045162\n",
    "\n",
    "    doc_length = get_doc_length(doc_id)\n",
    "        \n",
    "    \n",
    "    \"Find term frequency\"\n",
    "    \n",
    "    term_frequency = term_freq[term]\n",
    "    \n",
    "    \"Find document frequency\"\n",
    "    document_frequency = len(Merged_block[term])\n",
    "            \n",
    "    \n",
    "     \n",
    "    if document_frequency == 0:\n",
    "        #\"term doesn't exist in document\"\n",
    "        return 0\n",
    "    \n",
    "    \"Formula steps:\"\n",
    "    \n",
    "    idf = np.log(N/document_frequency)\n",
    "    tf = 1 + np.log(term_frequency)\n",
    "    weight = idf*tf\n",
    "    \n",
    "    \n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_collection_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This function can run a single query, multi-term AND query, and a multi-term AND query with 1 OR.\"\n",
    "\n",
    "def retrieve_docs(query,OR=False,vector_norm=False,tfidf=False,num_ret=10):\n",
    "    \n",
    "    terms = query.split(\" \")\n",
    "    temp_postings = []\n",
    "    postings_list = []\n",
    "    \"Run the following if its a single term query\"\n",
    "    \n",
    "    if len(terms) == 1:\n",
    "            \n",
    "        if terms[0] in Merged_block.keys():\n",
    "            documents = set(Merged_block[terms[0]])\n",
    "        else:\n",
    "            print(\"Term doesn't exist in the IR system.\")\n",
    "            return\n",
    "        \n",
    "        \"To store ranked documents\"\n",
    "        ranked_dict = {}\n",
    "        \n",
    "        \"Rank documents and store document-rankingweight mappings\"\n",
    "        \n",
    "        for document in documents:\n",
    "            if tfidf == False:\n",
    "                ranked_dict[document] = rank_doc(document,terms[0])\n",
    "            else:\n",
    "                ranked_dict[document] = rank_doc_tfidf(document,terms[0])\n",
    "\n",
    "\n",
    "        \n",
    "        \"Sort ranked results\"\n",
    "        ranked_dict = sorted(ranked_dict.items(), key=itemgetter(1),reverse=True)\n",
    "        \n",
    "        return ranked_dict\n",
    "        \n",
    "    \"Retrieve the postings of that term\"\n",
    "    for term in terms:\n",
    "        if (term) in Merged_block.keys():\n",
    "            postings_list = list(set(Merged_block[(term)]))\n",
    "                \n",
    "           \n",
    "        if len(postings_list) == 0:\n",
    "            print(\"Term not found in IR system, and thus will be ignored.\")\n",
    "            postings_list = []\n",
    "            continue\n",
    "        temp_postings.append(postings_list)    \n",
    "        \n",
    "        \n",
    "    \"Optimizes querying by dealing with the shortest postings lists first\"    \n",
    "    temp_postings.sort(key=len)\n",
    "    \n",
    "    documents = set()\n",
    "    \n",
    "    \"AND + OR logic\"\n",
    "    for i in range(len(temp_postings)):\n",
    "        if i == 0:\n",
    "            documents = documents.union(set(temp_postings[i]))\n",
    "        if i == 1 and OR == True:\n",
    "            documents = documents.union(set(temp_postings[i]))\n",
    "        else:\n",
    "            documents = documents.intersection(set(temp_postings[i]))\n",
    "     \n",
    "    \"Store document-rankingWeight mappings\"\n",
    "    ranked_dict = {}\n",
    "    \"Accumaltor for all the query terms in a doc as per the BM25 formula\"\n",
    "    total_rank = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        for term in terms:\n",
    "            if tfidf == False:\n",
    "                total_rank+= rank_doc(document,term)\n",
    "            else:\n",
    "                total_rank+= rank_doc_tfidf(document,term)\n",
    "\n",
    "\n",
    "        ranked_dict[document] = total_rank\n",
    "        \n",
    "    \"Sort ranked results\"       \n",
    "    ranked_dict = sorted(ranked_dict.items(), key=itemgetter(1),reverse=True)\n",
    "    \n",
    "    #return list(temp_dict.keys())[:10]\n",
    "    return ranked_dict[:num_ret]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Which departments have AI research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30841, 962.5606007696398),\n",
       " (26871, 949.8557620881151),\n",
       " (19951, 937.6610006131345),\n",
       " (24308, 924.973993656531),\n",
       " (13428, 912.2566733383641),\n",
       " (6514, 899.5412261015653),\n",
       " (12018, 887.3466477761266),\n",
       " (21744, 874.9471366234667),\n",
       " (14576, 862.2368429142776),\n",
       " (16239, 849.5207799523987)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = \"department artificial intelligence concordia research\" # concordia is important cuz other school could be referenced\n",
    "retrieve_docs((query1),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12478, 251.05167838678938),\n",
       " (26871, 225.65185976762305),\n",
       " (13781, 201.1616822888468),\n",
       " (26036, 175.7416580416431),\n",
       " (6514, 150.34236513415468),\n",
       " (6033, 125.85251807237526),\n",
       " (34637, 100.44675452371919),\n",
       " (21484, 75.32245975796543),\n",
       " (34918, 49.91578126184317),\n",
       " (13889, 25.425913549365845)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2 = \"department artificial intelligence research researcher engineering machine learning\"\n",
    "retrieve_docs((query2),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33565, 258.7621532100718),\n",
       " (11292, 238.69117445447435),\n",
       " (26871, 218.61039560194226),\n",
       " (24308, 199.23363627513785),\n",
       " (10259, 179.15232403933504),\n",
       " (6514, 159.1621234173823),\n",
       " (33326, 139.78561239771815),\n",
       " (10093, 119.70430016191533),\n",
       " (25004, 99.69530850394206),\n",
       " (1480, 79.61263681643665)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query3 = \"artificial intelligence ai research concordia faculty department university\"\n",
    "retrieve_docs((query3),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which researchers are working on AI research?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(26871, 157.5556548564489),\n",
       " (949, 131.85745874763847),\n",
       " (31283, 105.033512234989),\n",
       " (6514, 78.20942435382204),\n",
       " (6033, 52.511625661462865),\n",
       " (34918, 25.69782353034346)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = \"concordia computer science artificial intelligence engineering university researcher computing\"\n",
    "retrieve_docs((query1),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33790, 120.51758971570958),\n",
       " (26871, 105.28609946192421),\n",
       " (949, 90.52679888632926),\n",
       " (31283, 75.27287641477862),\n",
       " (6514, 60.01889337098953),\n",
       " (6033, 45.25977179364042),\n",
       " (23820, 30.010196888342495),\n",
       " (34918, 14.75913276458623)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2 = \"natural language processing researcher\"\n",
    "retrieve_docs((query2),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10102, 130.2351807893794),\n",
       " (949, 113.91024731162511),\n",
       " (31283, 97.59175352668551),\n",
       " (6033, 81.2731973865194),\n",
       " (11151, 64.95917841377683),\n",
       " (2958, 48.920818483518225),\n",
       " (12652, 32.59378634009286),\n",
       " (5412, 16.312412135657354)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query3 = \"researcher concordia deep learning phd engineering\" \n",
    "retrieve_docs((query3),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What AI research is being conducted at Concordia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4511, 107.08645939474773),\n",
       " (26871, 85.35389222125066),\n",
       " (15923, 64.14914183600933),\n",
       " (6514, 42.40889800254895),\n",
       " (34918, 21.20445872314173)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = \"concordia university artificial intelligence research paper graduate conducted\"\n",
    "retrieve_docs((query1),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18950, 107.58476591117675),\n",
       " (11460, 80.92900926271241),\n",
       " (10259, 53.62098267741364),\n",
       " (12018, 26.45185196966367)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2 = \"concordia artificial intelligence research neural network deep learning\"\n",
    "retrieve_docs((query2),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25599, 682.7112824985373),\n",
       " (11249, 676.1435224441726),\n",
       " (29169, 669.571428662156),\n",
       " (8678, 663.0034400408281),\n",
       " (1480, 656.4368182686878),\n",
       " (14787, 649.8662758566022),\n",
       " (21435, 643.2943771098338),\n",
       " (13243, 636.7223935670512),\n",
       " (13239, 630.1634296930455),\n",
       " (33206, 623.5962578882013)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query3 = \"ai research concordia\"\n",
    "retrieve_docs((query3),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
