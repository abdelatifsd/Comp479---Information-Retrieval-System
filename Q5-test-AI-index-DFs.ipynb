{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing AI index document frequencies in queries (made necessary code changes in rank_doc and rank_doc_tfidf)\n",
    "\n",
    "## To test it out, you have to manually alter these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re,os\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "import numpy as np\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Desktop\\\\479-A3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'A3-Q1-2-3-final.ipynb',\n",
       " 'ai_index_df',\n",
       " 'desktop.ini',\n",
       " 'documents-ai',\n",
       " 'documents-ai-2',\n",
       " 'documents-ai-3',\n",
       " 'documents-ai-filtered',\n",
       " 'documents-final',\n",
       " 'Filtering-duplicates.ipynb',\n",
       " 'final_index_concordia',\n",
       " 'Preparing-documents.ipynb',\n",
       " 'Q4-AI-index.ipynb',\n",
       " 'Q5.ipynb',\n",
       " 'term_freq_concordia',\n",
       " 'urls_run_final.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged_block = open(\"Final_index\")\n",
    "Merged_block = pickle.load(open(\"final_index_concordia\",mode='rb'))\n",
    "\n",
    "#Term frequency\n",
    "term_freq = pickle.load(open(\"term_freq_concordia\",mode=\"rb\"))\n",
    "\n",
    "#ai index\n",
    "ai_index = pickle.load(open(\"ai_index_df\",mode=\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['artificial', 'intelligence', 'machine', 'learning', 'ai', 'neural', 'networks', 'cognitive', 'supervised', 'unsupervised', 'semantic', 'analysis', 'chatbot', 'science', 'algorithm', 'data', 'mining', 'big', 'turing', 'analytics', 'cluster', 'engineering', 'reinforcement', 'nlp', 'deep', 'classification', 'regression', 'probability', 'gradient', 'machines'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Another approach to get length of a document.. Counts the number of times a token occured in some document(its length).\"\n",
    "def get_doc_length(doc_id):\n",
    "    \n",
    "    os.chdir(\"documents-final\")\n",
    "    f = open(str(doc_id)+\".txt\",\"r\",encoding=\"utf=8\")\n",
    "    text= f.read()\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "    return len(text.split(\" \"))\n",
    "\n",
    "\"Calculates the average length of all documents using the get_doc_length method.\"\n",
    "def get_collection_length():\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    os.chdir(\"documents-final\")\n",
    "    docs = os.listdir()\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "    for doc in docs:\n",
    "        if doc.split(\".\")[0].isdigit():\n",
    "            total+=get_doc_length(doc)\n",
    "    \n",
    "    \n",
    "        \n",
    "    return total/40211\n",
    "        \n",
    "\n",
    "\"BM25 Ranking formula implementation\"\n",
    "def rank_doc(doc_id,term):\n",
    "        \n",
    "    N = 17393 # Total documents number\n",
    "    k1 = 0.5 # tuning parameter\n",
    "    b = 0.5 # tuning parameter\n",
    "    postings_list = []\n",
    "    \n",
    "    avg_doc_length = 200.04749944045162\n",
    "\n",
    "\n",
    "    doc_length = get_doc_length(doc_id)\n",
    "        \n",
    "    \n",
    "    \"Find term frequency\"\n",
    "    \n",
    "    term_frequency = term_freq[term]\n",
    "    \n",
    "    \"Find document frequency\"\n",
    "    if term in ai_index.keys():\n",
    "        document_frequency = ai_index[term]\n",
    "    else:\n",
    "        document_frequency = len(Merged_block[term])\n",
    "    \n",
    "    \"Formula steps:\"\n",
    "    \n",
    "    idf = np.log(N/document_frequency)\n",
    "    numerator = (k1+1)*term_frequency\n",
    "    denomenator = k1*((1-b)+ (b*(doc_length/avg_doc_length))) + term_frequency\n",
    "    \n",
    "    ratio = numerator/denomenator\n",
    "    \n",
    "    weight = idf * ratio\n",
    "    \n",
    "    return weight\n",
    "\n",
    "def rank_doc_tfidf(doc_id,term):\n",
    "    \n",
    "    N = 17393 # Total documents number\n",
    "    postings_list = []\n",
    "    \n",
    "    avg_doc_length = 200.04749944045162\n",
    "\n",
    "    doc_length = get_doc_length(doc_id)\n",
    "        \n",
    "    \n",
    "    \"Find term frequency\"\n",
    "    \n",
    "    term_frequency = term_freq[term]\n",
    "    \n",
    "    \"Find document frequency\"\n",
    "    if term in ai_index.keys():\n",
    "        document_frequency = ai_index[term]\n",
    "    else:\n",
    "        document_frequency = len(Merged_block[term])\n",
    "            \n",
    "    \n",
    "     \n",
    "    if document_frequency == 0:\n",
    "        #\"term doesn't exist in document\"\n",
    "        return 0\n",
    "    \n",
    "    \"Formula steps:\"\n",
    "    \n",
    "    idf = np.log(N/document_frequency)\n",
    "    tf = 1 + np.log(term_frequency)\n",
    "    weight = idf*tf\n",
    "    \n",
    "    \n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This function can run a single query, multi-term AND query, and a multi-term AND query with 1 OR.\"\n",
    "\n",
    "def retrieve_docs(query,OR=False,vector_norm=False,tfidf=False,num_ret=10):\n",
    "    \n",
    "    terms = query.split(\" \")\n",
    "    temp_postings = []\n",
    "    postings_list = []\n",
    "    \"Run the following if its a single term query\"\n",
    "    \n",
    "    if len(terms) == 1:\n",
    "            \n",
    "        if terms[0] in Merged_block.keys():\n",
    "            documents = set(Merged_block[terms[0]])\n",
    "        else:\n",
    "            print(\"Term doesn't exist in the IR system.\")\n",
    "            return\n",
    "        \n",
    "        \"To store ranked documents\"\n",
    "        ranked_dict = {}\n",
    "        \n",
    "        \"Rank documents and store document-rankingweight mappings\"\n",
    "        \n",
    "        for document in documents:\n",
    "            if tfidf == False:\n",
    "                ranked_dict[document] = rank_doc(document,terms[0])\n",
    "            else:\n",
    "                ranked_dict[document] = rank_doc_tfidf(document,terms[0])\n",
    "\n",
    "\n",
    "        \n",
    "        \"Sort ranked results\"\n",
    "        ranked_dict = sorted(ranked_dict.items(), key=itemgetter(1),reverse=True)\n",
    "        \n",
    "        return ranked_dict\n",
    "        \n",
    "    \"Retrieve the postings of that term\"\n",
    "    for term in terms:\n",
    "        if (term) in Merged_block.keys():\n",
    "            postings_list = list(set(Merged_block[(term)]))\n",
    "                \n",
    "           \n",
    "        if len(postings_list) == 0:\n",
    "            print(\"Term not found in IR system, and thus will be ignored.\")\n",
    "            postings_list = []\n",
    "            continue\n",
    "        temp_postings.append(postings_list)    \n",
    "        \n",
    "        \n",
    "    \"Optimizes querying by dealing with the shortest postings lists first\"    \n",
    "    temp_postings.sort(key=len)\n",
    "    \n",
    "    documents = set()\n",
    "    \n",
    "    \"AND + OR logic\"\n",
    "    for i in range(len(temp_postings)):\n",
    "        if i == 0:\n",
    "            documents = documents.union(set(temp_postings[i]))\n",
    "        if i == 1 and OR == True:\n",
    "            documents = documents.union(set(temp_postings[i]))\n",
    "        else:\n",
    "            documents = documents.intersection(set(temp_postings[i]))\n",
    "     \n",
    "    \"Store document-rankingWeight mappings\"\n",
    "    ranked_dict = {}\n",
    "    \"Accumaltor for all the query terms in a doc as per the BM25 formula\"\n",
    "    total_rank = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        for term in terms:\n",
    "            if tfidf == False:\n",
    "                total_rank+= rank_doc(document,term)\n",
    "            else:\n",
    "                total_rank+= rank_doc_tfidf(document,term)\n",
    "\n",
    "\n",
    "        ranked_dict[document] = total_rank\n",
    "        \n",
    "    \"Sort ranked results\"       \n",
    "    ranked_dict = sorted(ranked_dict.items(), key=itemgetter(1),reverse=True)\n",
    "    \n",
    "    #return list(temp_dict.keys())[:10]\n",
    "    return ranked_dict[:num_ret]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"artificial intelligence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17402, 2897.974674452672),\n",
       " (22513, 2886.733611141023),\n",
       " (11249, 2875.473195625473),\n",
       " (14832, 2864.212962234033),\n",
       " (19951, 2852.9492674494963),\n",
       " (20461, 2841.73772648784),\n",
       " (29676, 2830.475833519786),\n",
       " (21484, 2819.2242945687285),\n",
       " (8678, 2807.9841587674196),\n",
       " (12261, 2796.750007675136)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old df weights\n",
    "retrieve_docs((query1),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17402, 1215.959398262353),\n",
       " (22513, 1211.2427910878355),\n",
       " (11249, 1206.5181237647414),\n",
       " (14832, 1201.7935322950038),\n",
       " (19951, 1197.067499183405),\n",
       " (20461, 1192.3631883795106),\n",
       " (29676, 1187.6379057089262),\n",
       " (21484, 1182.91693543015),\n",
       " (8678, 1178.2007145665102),\n",
       " (12261, 1173.4869863632612)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ai index df weights\n",
    "retrieve_docs((query1),tfidf=False,num_ret=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
